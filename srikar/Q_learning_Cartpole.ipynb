{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytlh2rLTP6ce",
        "outputId": "79df2a9d-9de0-422a-b4c0-27c41509bc5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import time\n",
        "import math\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "print(env.action_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE=0.1\n",
        "\n",
        "DISCOUNT=0.95\n",
        "EPISODES=100000\n",
        "total=0\n",
        "total_reward=0\n",
        "prior_reward=0\n",
        "\n",
        "Observation = [30,30,50,50]\n",
        "np_array_win_size= np.array([0.25,0.25,0.1,0.1])\n",
        "\n",
        "epsilon=0.6\n",
        "\n",
        "#epsilon_decay_value = 0.99995"
      ],
      "metadata": {
        "id": "We5e_Q-0QA7f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_table = np.random.uniform(low=0, high=1, size=(Observation + [env.action_space.n]))\n",
        "q_table.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvB3QWD7QDYv",
        "outputId": "90bd32ea-7916-4767-90eb-c8ef0c0dfdf4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 30, 50, 50, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_discrete_state(state):\n",
        "  discrete_state = state/np_array_win_size+ np.array([15,10,1,10])\n",
        "  return tuple(discrete_state.astype(np.int))"
      ],
      "metadata": {
        "id": "NZMwKUIZQFCv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max=0\n",
        "for episode in range(EPISODES + 1):\n",
        "  t0 = time.time()\n",
        "  discrete_state=get_discrete_state(env.reset())\n",
        "  done=False\n",
        "  episode_reward=0\n",
        "  timet = 0 \n",
        "  if episode %2000 ==0:\n",
        "    print(\"Episode: \" + str(episode))\n",
        "  while not done:\n",
        "    #env.render()\n",
        "    timet+=1\n",
        "    if np.random.random() > epsilon:\n",
        "      action = np.argmax(q_table[discrete_state])\n",
        "    else:\n",
        "      action = np.random.randint(0, env.action_space.n)\n",
        "    \n",
        "    new_state, reward, done, _=env.step(action)\n",
        "\n",
        "    episode_reward += reward\n",
        "\n",
        "    new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "    if not done:\n",
        "      max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "      current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "      new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "      q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "    discrete_state = new_discrete_state\n",
        "\n",
        "  \"\"\"if epsilon>0.05:\n",
        "    if episode_reward > prior_reward and episode >10000:\n",
        "      epsilon = math.pow(epsilon_decay_value, episode - 10000)\n",
        "\n",
        "      if episode % 500 ==0:\n",
        "        print(\"Epsilon: \" + str(epsilon))\"\"\"\n",
        "\n",
        "  t1=time.time()\n",
        "  episode_total =t1-t0\n",
        "  total = total + episode_total\n",
        "\n",
        "  total_reward += episode_reward\n",
        "  prior_reward = episode_reward\n",
        "\n",
        "  if max < episode_reward:\n",
        "    max = episode_reward\n",
        "\n",
        "  if episode % 1000 == 0:\n",
        "    mean = total/1000\n",
        "    print(\"Time Average: \"+ str(mean))\n",
        "    total=0\n",
        "\n",
        "    print(\"Time = \"+ str(timet))\n",
        "\n",
        "    mean_reward = total_reward/1000\n",
        "    print(\"Mean Reward: \"+str(episode_reward))\n",
        "    total_reward = 0\n",
        "\n",
        "print(\"-------------------------\")\n",
        "print(max)\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfSq9X4TQGxT",
        "outputId": "f2f75fce-7be7-4575-bb1c-3abfa5393c7f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0\n",
            "Time Average: 7.0774555206298825e-06\n",
            "Time = 37\n",
            "Mean Reward: 37.0\n",
            "Time Average: 0.0013333208560943604\n",
            "Time = 24\n",
            "Mean Reward: 24.0\n",
            "Episode: 2000\n",
            "Time Average: 0.0017693464756011962\n",
            "Time = 20\n",
            "Mean Reward: 20.0\n",
            "Time Average: 0.0021216933727264404\n",
            "Time = 31\n",
            "Mean Reward: 31.0\n",
            "Episode: 4000\n",
            "Time Average: 0.002098104476928711\n",
            "Time = 50\n",
            "Mean Reward: 50.0\n",
            "Time Average: 0.0022597827911376954\n",
            "Time = 42\n",
            "Mean Reward: 42.0\n",
            "Episode: 6000\n",
            "Time Average: 0.002278067588806152\n",
            "Time = 21\n",
            "Mean Reward: 21.0\n",
            "Time Average: 0.002275777578353882\n",
            "Time = 95\n",
            "Mean Reward: 95.0\n",
            "Episode: 8000\n",
            "Time Average: 0.0028195960521698\n",
            "Time = 98\n",
            "Mean Reward: 98.0\n",
            "Time Average: 0.002365692853927612\n",
            "Time = 51\n",
            "Mean Reward: 51.0\n",
            "Episode: 10000\n",
            "Time Average: 0.0022581303119659423\n",
            "Time = 55\n",
            "Mean Reward: 55.0\n",
            "Time Average: 0.0023076391220092775\n",
            "Time = 103\n",
            "Mean Reward: 103.0\n",
            "Episode: 12000\n",
            "Time Average: 0.002581851005554199\n",
            "Time = 76\n",
            "Mean Reward: 76.0\n",
            "Time Average: 0.0025618247985839844\n",
            "Time = 58\n",
            "Mean Reward: 58.0\n",
            "Episode: 14000\n",
            "Time Average: 0.002764294624328613\n",
            "Time = 48\n",
            "Mean Reward: 48.0\n",
            "Time Average: 0.0026031434535980225\n",
            "Time = 18\n",
            "Mean Reward: 18.0\n",
            "Episode: 16000\n",
            "Time Average: 0.002708695411682129\n",
            "Time = 135\n",
            "Mean Reward: 135.0\n",
            "Time Average: 0.0025443880558013916\n",
            "Time = 56\n",
            "Mean Reward: 56.0\n",
            "Episode: 18000\n",
            "Time Average: 0.002399998426437378\n",
            "Time = 67\n",
            "Mean Reward: 67.0\n",
            "Time Average: 0.002397943019866943\n",
            "Time = 19\n",
            "Mean Reward: 19.0\n",
            "Episode: 20000\n",
            "Time Average: 0.002389281988143921\n",
            "Time = 73\n",
            "Mean Reward: 73.0\n",
            "Time Average: 0.0024072248935699462\n",
            "Time = 39\n",
            "Mean Reward: 39.0\n",
            "Episode: 22000\n",
            "Time Average: 0.002387149810791016\n",
            "Time = 27\n",
            "Mean Reward: 27.0\n",
            "Time Average: 0.0024538235664367677\n",
            "Time = 21\n",
            "Mean Reward: 21.0\n",
            "Episode: 24000\n",
            "Time Average: 0.0024792869091033935\n",
            "Time = 133\n",
            "Mean Reward: 133.0\n",
            "Time Average: 0.0023504569530487063\n",
            "Time = 13\n",
            "Mean Reward: 13.0\n",
            "Episode: 26000\n",
            "Time Average: 0.0025420353412628173\n",
            "Time = 17\n",
            "Mean Reward: 17.0\n",
            "Time Average: 0.002446713447570801\n",
            "Time = 91\n",
            "Mean Reward: 91.0\n",
            "Episode: 28000\n",
            "Time Average: 0.002537531852722168\n",
            "Time = 59\n",
            "Mean Reward: 59.0\n",
            "Time Average: 0.0024310925006866455\n",
            "Time = 168\n",
            "Mean Reward: 168.0\n",
            "Episode: 30000\n",
            "Time Average: 0.002597460746765137\n",
            "Time = 65\n",
            "Mean Reward: 65.0\n",
            "Time Average: 0.00255035662651062\n",
            "Time = 49\n",
            "Mean Reward: 49.0\n",
            "Episode: 32000\n",
            "Time Average: 0.002555978536605835\n",
            "Time = 48\n",
            "Mean Reward: 48.0\n",
            "Time Average: 0.0025774307250976563\n",
            "Time = 22\n",
            "Mean Reward: 22.0\n",
            "Episode: 34000\n",
            "Time Average: 0.002597217321395874\n",
            "Time = 65\n",
            "Mean Reward: 65.0\n",
            "Time Average: 0.0026578116416931153\n",
            "Time = 87\n",
            "Mean Reward: 87.0\n",
            "Episode: 36000\n",
            "Time Average: 0.002577996015548706\n",
            "Time = 51\n",
            "Mean Reward: 51.0\n",
            "Time Average: 0.0025224602222442627\n",
            "Time = 86\n",
            "Mean Reward: 86.0\n",
            "Episode: 38000\n",
            "Time Average: 0.0025146894454956053\n",
            "Time = 10\n",
            "Mean Reward: 10.0\n",
            "Time Average: 0.002632845640182495\n",
            "Time = 109\n",
            "Mean Reward: 109.0\n",
            "Episode: 40000\n",
            "Time Average: 0.0025792372226715087\n",
            "Time = 43\n",
            "Mean Reward: 43.0\n",
            "Time Average: 0.0025359861850738526\n",
            "Time = 24\n",
            "Mean Reward: 24.0\n",
            "Episode: 42000\n",
            "Time Average: 0.0026978375911712645\n",
            "Time = 72\n",
            "Mean Reward: 72.0\n",
            "Time Average: 0.002603237867355347\n",
            "Time = 121\n",
            "Mean Reward: 121.0\n",
            "Episode: 44000\n",
            "Time Average: 0.0026395015716552733\n",
            "Time = 99\n",
            "Mean Reward: 99.0\n",
            "Time Average: 0.002708047866821289\n",
            "Time = 88\n",
            "Mean Reward: 88.0\n",
            "Episode: 46000\n",
            "Time Average: 0.002743377685546875\n",
            "Time = 180\n",
            "Mean Reward: 180.0\n",
            "Time Average: 0.002851177215576172\n",
            "Time = 21\n",
            "Mean Reward: 21.0\n",
            "Episode: 48000\n",
            "Time Average: 0.0025655333995819093\n",
            "Time = 49\n",
            "Mean Reward: 49.0\n",
            "Time Average: 0.002554042100906372\n",
            "Time = 22\n",
            "Mean Reward: 22.0\n",
            "Episode: 50000\n",
            "Time Average: 0.0026244220733642578\n",
            "Time = 53\n",
            "Mean Reward: 53.0\n",
            "Time Average: 0.00263543701171875\n",
            "Time = 58\n",
            "Mean Reward: 58.0\n",
            "Episode: 52000\n",
            "Time Average: 0.0026958639621734617\n",
            "Time = 157\n",
            "Mean Reward: 157.0\n",
            "Time Average: 0.0027765438556671142\n",
            "Time = 45\n",
            "Mean Reward: 45.0\n",
            "Episode: 54000\n",
            "Time Average: 0.002678682088851929\n",
            "Time = 84\n",
            "Mean Reward: 84.0\n",
            "Time Average: 0.0027571570873260497\n",
            "Time = 74\n",
            "Mean Reward: 74.0\n",
            "Episode: 56000\n",
            "Time Average: 0.0026372478008270263\n",
            "Time = 60\n",
            "Mean Reward: 60.0\n",
            "Time Average: 0.002742103338241577\n",
            "Time = 56\n",
            "Mean Reward: 56.0\n",
            "Episode: 58000\n",
            "Time Average: 0.0026079375743865967\n",
            "Time = 137\n",
            "Mean Reward: 137.0\n",
            "Time Average: 0.002695617437362671\n",
            "Time = 58\n",
            "Mean Reward: 58.0\n",
            "Episode: 60000\n",
            "Time Average: 0.0027104427814483643\n",
            "Time = 79\n",
            "Mean Reward: 79.0\n",
            "Time Average: 0.002789909839630127\n",
            "Time = 82\n",
            "Mean Reward: 82.0\n",
            "Episode: 62000\n",
            "Time Average: 0.002747530937194824\n",
            "Time = 134\n",
            "Mean Reward: 134.0\n",
            "Time Average: 0.0026033990383148194\n",
            "Time = 31\n",
            "Mean Reward: 31.0\n",
            "Episode: 64000\n",
            "Time Average: 0.0026812839508056643\n",
            "Time = 61\n",
            "Mean Reward: 61.0\n",
            "Time Average: 0.0026701040267944334\n",
            "Time = 52\n",
            "Mean Reward: 52.0\n",
            "Episode: 66000\n",
            "Time Average: 0.0026754517555236815\n",
            "Time = 89\n",
            "Mean Reward: 89.0\n",
            "Time Average: 0.0026356861591339113\n",
            "Time = 27\n",
            "Mean Reward: 27.0\n",
            "Episode: 68000\n",
            "Time Average: 0.002729011058807373\n",
            "Time = 71\n",
            "Mean Reward: 71.0\n",
            "Time Average: 0.0027389521598815917\n",
            "Time = 66\n",
            "Mean Reward: 66.0\n",
            "Episode: 70000\n",
            "Time Average: 0.002693774461746216\n",
            "Time = 83\n",
            "Mean Reward: 83.0\n",
            "Time Average: 0.0027431254386901856\n",
            "Time = 60\n",
            "Mean Reward: 60.0\n",
            "Episode: 72000\n",
            "Time Average: 0.0027188057899475097\n",
            "Time = 16\n",
            "Mean Reward: 16.0\n",
            "Time Average: 0.00345316743850708\n",
            "Time = 29\n",
            "Mean Reward: 29.0\n",
            "Episode: 74000\n",
            "Time Average: 0.002817622184753418\n",
            "Time = 20\n",
            "Mean Reward: 20.0\n",
            "Time Average: 0.003038203239440918\n",
            "Time = 117\n",
            "Mean Reward: 117.0\n",
            "Episode: 76000\n",
            "Time Average: 0.0028844430446624755\n",
            "Time = 71\n",
            "Mean Reward: 71.0\n",
            "Time Average: 0.002947788715362549\n",
            "Time = 100\n",
            "Mean Reward: 100.0\n",
            "Episode: 78000\n",
            "Time Average: 0.002888702630996704\n",
            "Time = 133\n",
            "Mean Reward: 133.0\n",
            "Time Average: 0.0026877858638763427\n",
            "Time = 91\n",
            "Mean Reward: 91.0\n",
            "Episode: 80000\n",
            "Time Average: 0.002681168556213379\n",
            "Time = 100\n",
            "Mean Reward: 100.0\n",
            "Time Average: 0.0026391379833221435\n",
            "Time = 107\n",
            "Mean Reward: 107.0\n",
            "Episode: 82000\n",
            "Time Average: 0.002829574108123779\n",
            "Time = 90\n",
            "Mean Reward: 90.0\n",
            "Time Average: 0.002626786708831787\n",
            "Time = 78\n",
            "Mean Reward: 78.0\n",
            "Episode: 84000\n",
            "Time Average: 0.0028198635578155517\n",
            "Time = 97\n",
            "Mean Reward: 97.0\n",
            "Time Average: 0.0026689019203186034\n",
            "Time = 102\n",
            "Mean Reward: 102.0\n",
            "Episode: 86000\n",
            "Time Average: 0.0027462267875671387\n",
            "Time = 22\n",
            "Mean Reward: 22.0\n",
            "Time Average: 0.0027021996974945068\n",
            "Time = 90\n",
            "Mean Reward: 90.0\n",
            "Episode: 88000\n",
            "Time Average: 0.0027957279682159423\n",
            "Time = 53\n",
            "Mean Reward: 53.0\n",
            "Time Average: 0.0027097318172454835\n",
            "Time = 46\n",
            "Mean Reward: 46.0\n",
            "Episode: 90000\n",
            "Time Average: 0.002655488967895508\n",
            "Time = 60\n",
            "Mean Reward: 60.0\n",
            "Time Average: 0.002810004472732544\n",
            "Time = 16\n",
            "Mean Reward: 16.0\n",
            "Episode: 92000\n",
            "Time Average: 0.002714499235153198\n",
            "Time = 125\n",
            "Mean Reward: 125.0\n",
            "Time Average: 0.002719463586807251\n",
            "Time = 47\n",
            "Mean Reward: 47.0\n",
            "Episode: 94000\n",
            "Time Average: 0.0028204734325408937\n",
            "Time = 107\n",
            "Mean Reward: 107.0\n",
            "Time Average: 0.0027356410026550294\n",
            "Time = 58\n",
            "Mean Reward: 58.0\n",
            "Episode: 96000\n",
            "Time Average: 0.0027077336311340334\n",
            "Time = 68\n",
            "Mean Reward: 68.0\n",
            "Time Average: 0.0025624492168426515\n",
            "Time = 99\n",
            "Mean Reward: 99.0\n",
            "Episode: 98000\n",
            "Time Average: 0.0027888338565826417\n",
            "Time = 37\n",
            "Mean Reward: 37.0\n",
            "Time Average: 0.003075854778289795\n",
            "Time = 51\n",
            "Mean Reward: 51.0\n",
            "Episode: 100000\n",
            "Time Average: 0.0028093485832214357\n",
            "Time = 56\n",
            "Mean Reward: 56.0\n",
            "-------------------------\n",
            "383.0\n"
          ]
        }
      ]
    }
  ]
}