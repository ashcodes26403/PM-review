{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b415816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a348c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(x, scores):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0898f360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOMemory:\n",
    "  def __init__(self, batch_size):\n",
    "    self.states = []\n",
    "    self.probs = []\n",
    "    self.vals = []\n",
    "    self.actions = []\n",
    "    self.rewards = []\n",
    "    self.dones = []\n",
    "\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  def generate_batches(self):\n",
    "    n_states = len(self.states)\n",
    "    batch_start = np.arange(0, n_states, self.batch_size)\n",
    "    indices = np.arange(n_states, dtype = np.int64)\n",
    "    np.random.shuffle(indices)\n",
    "    batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "    return np.array(self.states),np.array(self.actions),np.array(self.probs),np.array(self.vals),np.array(self.rewards),np.array(self.dones),batches\n",
    "\n",
    "  def store_memory(self, state, action, probs, vals, reward, done):\n",
    "    self.states.append(state)\n",
    "    self.actions.append(action)\n",
    "    self.probs.append(probs)\n",
    "    self.vals.append(vals)\n",
    "    self.rewards.append(reward)\n",
    "    self.dones.append(done)\n",
    "\n",
    "  def clear_memory(self):\n",
    "    self.states = []\n",
    "    self.probs = []\n",
    "    self.actions = []\n",
    "    self.rewards = []\n",
    "    self.dones = []\n",
    "    self.vals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb560135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, n_actions, input_dims, alpha,\n",
    "            fc1_dims=256, fc2_dims=256, chkpt_dir='tmp/ppo'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "       # self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "        self.actor = nn.Sequential(\n",
    "                nn.Linear(*input_dims, fc1_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc1_dims, fc2_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc2_dims, n_actions),\n",
    "                nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        dist = self.actor(state)\n",
    "        dist = Categorical(dist)\n",
    "        \n",
    "        return dist\n",
    "\n",
    "   # def save_checkpoint(self):\n",
    "   #     T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "   # def load_checkpoint(self):\n",
    "       # self.load_state_dict(T.load(self.checkpoint_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52eefc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, alpha, fc1_dims=256, fc2_dims=256,\n",
    "            chkpt_dir='tmp/ppo'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "       # self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(*input_dims, fc1_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc1_dims, fc2_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc2_dims, 1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "\n",
    "        return value\n",
    "\n",
    "    #def save_checkpoint(self):\n",
    "        #T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "   # def load_checkpoint(self):\n",
    "        s#elf.load_state_dict(T.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13848e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, n_actions, input_dims, gamma=0.99, alpha=0.0003, gae_lambda=0.95,\n",
    "            policy_clip=0.2, batch_size=64, n_epochs=10):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.actor = ActorNetwork(n_actions, input_dims, alpha)\n",
    "        self.critic = CriticNetwork(input_dims, alpha)\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "       \n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    #def save_models(self):\n",
    "       # print('... saving models ...')\n",
    "        #self.actor.save_checkpoint()\n",
    "        #self.critic.save_checkpoint()\n",
    "\n",
    "    #def load_models(self):\n",
    "     #   print('... loading models ...')\n",
    "      #  self.actor.load_checkpoint()\n",
    "       # self.critic.load_checkpoint()\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "\n",
    "        dist = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        probs = T.squeeze(dist.log_prob(action)).item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "\n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr,\\\n",
    "            reward_arr, dones_arr, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "\n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*\\\n",
    "                            (1-int(dones_arr[k])) - values[k])\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[t] = a_t\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)\n",
    "                old_probs = T.tensor(old_prob_arr[batch]).to(self.actor.device)\n",
    "                actions = T.tensor(action_arr[batch]).to(self.actor.device)\n",
    "\n",
    "                dist = self.actor(states)\n",
    "                critic_value = self.critic(states)\n",
    "\n",
    "                critic_value = T.squeeze(critic_value)\n",
    "\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "                #prob_ratio = (new_probs - old_probs).exp()\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage[batch]\n",
    "                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "\n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "\n",
    "        self.memory.clear_memory()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "674435a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99067c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashmit Sinha\\AppData\\Local\\Temp\\ipykernel_20232\\3495565209.py:27: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 score 11.0 avg score 11.0 time_steps 11 learning_steps 0\n",
      "episode 1 score 43.0 avg score 27.0 time_steps 54 learning_steps 2\n",
      "episode 2 score 34.0 avg score 29.3 time_steps 88 learning_steps 4\n",
      "episode 3 score 19.0 avg score 26.8 time_steps 107 learning_steps 5\n",
      "episode 4 score 19.0 avg score 25.2 time_steps 126 learning_steps 6\n",
      "episode 5 score 16.0 avg score 23.7 time_steps 142 learning_steps 7\n",
      "episode 6 score 15.0 avg score 22.4 time_steps 157 learning_steps 7\n",
      "episode 7 score 11.0 avg score 21.0 time_steps 168 learning_steps 8\n",
      "episode 8 score 11.0 avg score 19.9 time_steps 179 learning_steps 8\n",
      "episode 9 score 18.0 avg score 19.7 time_steps 197 learning_steps 9\n",
      "episode 10 score 22.0 avg score 19.9 time_steps 219 learning_steps 10\n",
      "episode 11 score 22.0 avg score 20.1 time_steps 241 learning_steps 12\n",
      "episode 12 score 17.0 avg score 19.8 time_steps 258 learning_steps 12\n",
      "episode 13 score 21.0 avg score 19.9 time_steps 279 learning_steps 13\n",
      "episode 14 score 41.0 avg score 21.3 time_steps 320 learning_steps 16\n",
      "episode 15 score 16.0 avg score 21.0 time_steps 336 learning_steps 16\n",
      "episode 16 score 15.0 avg score 20.6 time_steps 351 learning_steps 17\n",
      "episode 17 score 18.0 avg score 20.5 time_steps 369 learning_steps 18\n",
      "episode 18 score 12.0 avg score 20.1 time_steps 381 learning_steps 19\n",
      "episode 19 score 27.0 avg score 20.4 time_steps 408 learning_steps 20\n",
      "episode 20 score 21.0 avg score 20.4 time_steps 429 learning_steps 21\n",
      "episode 21 score 30.0 avg score 20.9 time_steps 459 learning_steps 22\n",
      "episode 22 score 41.0 avg score 21.7 time_steps 500 learning_steps 25\n",
      "episode 23 score 153.0 avg score 27.2 time_steps 653 learning_steps 32\n",
      "episode 24 score 21.0 avg score 27.0 time_steps 674 learning_steps 33\n",
      "episode 25 score 48.0 avg score 27.8 time_steps 722 learning_steps 36\n",
      "episode 26 score 78.0 avg score 29.6 time_steps 800 learning_steps 40\n",
      "episode 27 score 54.0 avg score 30.5 time_steps 854 learning_steps 42\n",
      "episode 28 score 29.0 avg score 30.4 time_steps 883 learning_steps 44\n",
      "episode 29 score 51.0 avg score 31.1 time_steps 934 learning_steps 46\n",
      "episode 30 score 97.0 avg score 33.3 time_steps 1031 learning_steps 51\n",
      "episode 31 score 23.0 avg score 32.9 time_steps 1054 learning_steps 52\n",
      "episode 32 score 101.0 avg score 35.0 time_steps 1155 learning_steps 57\n",
      "episode 33 score 109.0 avg score 37.2 time_steps 1264 learning_steps 63\n",
      "episode 34 score 21.0 avg score 36.7 time_steps 1285 learning_steps 64\n",
      "episode 35 score 107.0 avg score 38.7 time_steps 1392 learning_steps 69\n",
      "episode 36 score 44.0 avg score 38.8 time_steps 1436 learning_steps 71\n",
      "episode 37 score 190.0 avg score 42.8 time_steps 1626 learning_steps 81\n",
      "episode 38 score 92.0 avg score 44.1 time_steps 1718 learning_steps 85\n",
      "episode 39 score 200.0 avg score 48.0 time_steps 1918 learning_steps 95\n",
      "episode 40 score 145.0 avg score 50.3 time_steps 2063 learning_steps 103\n",
      "episode 41 score 105.0 avg score 51.6 time_steps 2168 learning_steps 108\n",
      "episode 42 score 82.0 avg score 52.3 time_steps 2250 learning_steps 112\n",
      "episode 43 score 75.0 avg score 52.8 time_steps 2325 learning_steps 116\n",
      "episode 44 score 51.0 avg score 52.8 time_steps 2376 learning_steps 118\n",
      "episode 45 score 106.0 avg score 54.0 time_steps 2482 learning_steps 124\n",
      "episode 46 score 38.0 avg score 53.6 time_steps 2520 learning_steps 126\n",
      "episode 47 score 28.0 avg score 53.1 time_steps 2548 learning_steps 127\n",
      "episode 48 score 25.0 avg score 52.5 time_steps 2573 learning_steps 128\n",
      "episode 49 score 110.0 avg score 53.7 time_steps 2683 learning_steps 134\n",
      "episode 50 score 147.0 avg score 55.5 time_steps 2830 learning_steps 141\n",
      "episode 51 score 200.0 avg score 58.3 time_steps 3030 learning_steps 151\n",
      "episode 52 score 200.0 avg score 60.9 time_steps 3230 learning_steps 161\n",
      "episode 53 score 155.0 avg score 62.7 time_steps 3385 learning_steps 169\n",
      "episode 54 score 85.0 avg score 63.1 time_steps 3470 learning_steps 173\n",
      "episode 55 score 200.0 avg score 65.5 time_steps 3670 learning_steps 183\n",
      "episode 56 score 171.0 avg score 67.4 time_steps 3841 learning_steps 192\n",
      "episode 57 score 154.0 avg score 68.9 time_steps 3995 learning_steps 199\n",
      "episode 58 score 200.0 avg score 71.1 time_steps 4195 learning_steps 209\n",
      "episode 59 score 181.0 avg score 72.9 time_steps 4376 learning_steps 218\n",
      "episode 60 score 200.0 avg score 75.0 time_steps 4576 learning_steps 228\n",
      "episode 61 score 200.0 avg score 77.0 time_steps 4776 learning_steps 238\n",
      "episode 62 score 200.0 avg score 79.0 time_steps 4976 learning_steps 248\n",
      "episode 63 score 132.0 avg score 79.8 time_steps 5108 learning_steps 255\n",
      "episode 64 score 32.0 avg score 79.1 time_steps 5140 learning_steps 257\n",
      "episode 65 score 37.0 avg score 78.4 time_steps 5177 learning_steps 258\n",
      "episode 66 score 136.0 avg score 79.3 time_steps 5313 learning_steps 265\n",
      "episode 67 score 165.0 avg score 80.6 time_steps 5478 learning_steps 273\n",
      "episode 68 score 129.0 avg score 81.3 time_steps 5607 learning_steps 280\n",
      "episode 69 score 144.0 avg score 82.2 time_steps 5751 learning_steps 287\n",
      "episode 70 score 128.0 avg score 82.8 time_steps 5879 learning_steps 293\n",
      "episode 71 score 150.0 avg score 83.7 time_steps 6029 learning_steps 301\n",
      "episode 72 score 52.0 avg score 83.3 time_steps 6081 learning_steps 304\n",
      "episode 73 score 79.0 avg score 83.2 time_steps 6160 learning_steps 308\n",
      "episode 74 score 98.0 avg score 83.4 time_steps 6258 learning_steps 312\n",
      "episode 75 score 61.0 avg score 83.1 time_steps 6319 learning_steps 315\n",
      "episode 76 score 36.0 avg score 82.5 time_steps 6355 learning_steps 317\n",
      "episode 77 score 50.0 avg score 82.1 time_steps 6405 learning_steps 320\n",
      "episode 78 score 194.0 avg score 83.5 time_steps 6599 learning_steps 329\n",
      "episode 79 score 195.0 avg score 84.9 time_steps 6794 learning_steps 339\n",
      "episode 80 score 133.0 avg score 85.5 time_steps 6927 learning_steps 346\n",
      "episode 81 score 200.0 avg score 86.9 time_steps 7127 learning_steps 356\n",
      "episode 82 score 113.0 avg score 87.2 time_steps 7240 learning_steps 362\n",
      "episode 83 score 87.0 avg score 87.2 time_steps 7327 learning_steps 366\n",
      "episode 84 score 37.0 avg score 86.6 time_steps 7364 learning_steps 368\n",
      "episode 85 score 151.0 avg score 87.4 time_steps 7515 learning_steps 375\n",
      "episode 86 score 56.0 avg score 87.0 time_steps 7571 learning_steps 378\n",
      "episode 87 score 125.0 avg score 87.5 time_steps 7696 learning_steps 384\n",
      "episode 88 score 154.0 avg score 88.2 time_steps 7850 learning_steps 392\n",
      "episode 89 score 81.0 avg score 88.1 time_steps 7931 learning_steps 396\n",
      "episode 90 score 129.0 avg score 88.6 time_steps 8060 learning_steps 403\n",
      "episode 91 score 74.0 avg score 88.4 time_steps 8134 learning_steps 406\n",
      "episode 92 score 62.0 avg score 88.1 time_steps 8196 learning_steps 409\n",
      "episode 93 score 99.0 avg score 88.2 time_steps 8295 learning_steps 414\n",
      "episode 94 score 183.0 avg score 89.2 time_steps 8478 learning_steps 423\n",
      "episode 95 score 87.0 avg score 89.2 time_steps 8565 learning_steps 428\n",
      "episode 96 score 60.0 avg score 88.9 time_steps 8625 learning_steps 431\n",
      "episode 97 score 196.0 avg score 90.0 time_steps 8821 learning_steps 441\n",
      "episode 98 score 49.0 avg score 89.6 time_steps 8870 learning_steps 443\n",
      "episode 99 score 200.0 avg score 90.7 time_steps 9070 learning_steps 453\n",
      "episode 100 score 200.0 avg score 92.6 time_steps 9270 learning_steps 463\n",
      "episode 101 score 200.0 avg score 94.2 time_steps 9470 learning_steps 473\n",
      "episode 102 score 200.0 avg score 95.8 time_steps 9670 learning_steps 483\n",
      "episode 103 score 200.0 avg score 97.6 time_steps 9870 learning_steps 493\n",
      "episode 104 score 200.0 avg score 99.4 time_steps 10070 learning_steps 503\n",
      "episode 105 score 200.0 avg score 101.3 time_steps 10270 learning_steps 513\n",
      "episode 106 score 200.0 avg score 103.1 time_steps 10470 learning_steps 523\n",
      "episode 107 score 200.0 avg score 105.0 time_steps 10670 learning_steps 533\n",
      "episode 108 score 200.0 avg score 106.9 time_steps 10870 learning_steps 543\n",
      "episode 109 score 115.0 avg score 107.9 time_steps 10985 learning_steps 549\n",
      "episode 110 score 200.0 avg score 109.7 time_steps 11185 learning_steps 559\n",
      "episode 111 score 200.0 avg score 111.4 time_steps 11385 learning_steps 569\n",
      "episode 112 score 126.0 avg score 112.5 time_steps 11511 learning_steps 575\n",
      "episode 113 score 126.0 avg score 113.6 time_steps 11637 learning_steps 581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 114 score 200.0 avg score 115.2 time_steps 11837 learning_steps 591\n",
      "episode 115 score 200.0 avg score 117.0 time_steps 12037 learning_steps 601\n",
      "episode 116 score 200.0 avg score 118.9 time_steps 12237 learning_steps 611\n",
      "episode 117 score 184.0 avg score 120.5 time_steps 12421 learning_steps 621\n",
      "episode 118 score 25.0 avg score 120.7 time_steps 12446 learning_steps 622\n",
      "episode 119 score 111.0 avg score 121.5 time_steps 12557 learning_steps 627\n",
      "episode 120 score 200.0 avg score 123.3 time_steps 12757 learning_steps 637\n",
      "episode 121 score 41.0 avg score 123.4 time_steps 12798 learning_steps 639\n",
      "episode 122 score 200.0 avg score 125.0 time_steps 12998 learning_steps 649\n",
      "episode 123 score 200.0 avg score 125.5 time_steps 13198 learning_steps 659\n",
      "episode 124 score 200.0 avg score 127.2 time_steps 13398 learning_steps 669\n",
      "episode 125 score 200.0 avg score 128.8 time_steps 13598 learning_steps 679\n",
      "episode 126 score 162.0 avg score 129.6 time_steps 13760 learning_steps 688\n",
      "episode 127 score 126.0 avg score 130.3 time_steps 13886 learning_steps 694\n",
      "episode 128 score 113.0 avg score 131.2 time_steps 13999 learning_steps 699\n",
      "episode 129 score 200.0 avg score 132.7 time_steps 14199 learning_steps 709\n",
      "episode 130 score 200.0 avg score 133.7 time_steps 14399 learning_steps 719\n",
      "episode 131 score 200.0 avg score 135.4 time_steps 14599 learning_steps 729\n",
      "episode 132 score 200.0 avg score 136.4 time_steps 14799 learning_steps 739\n",
      "episode 133 score 200.0 avg score 137.3 time_steps 14999 learning_steps 749\n",
      "episode 134 score 200.0 avg score 139.1 time_steps 15199 learning_steps 759\n",
      "episode 135 score 66.0 avg score 138.7 time_steps 15265 learning_steps 763\n",
      "episode 136 score 200.0 avg score 140.3 time_steps 15465 learning_steps 773\n",
      "episode 137 score 109.0 avg score 139.5 time_steps 15574 learning_steps 778\n",
      "episode 138 score 61.0 avg score 139.2 time_steps 15635 learning_steps 781\n",
      "episode 139 score 105.0 avg score 138.2 time_steps 15740 learning_steps 787\n",
      "episode 140 score 200.0 avg score 138.8 time_steps 15940 learning_steps 797\n",
      "episode 141 score 200.0 avg score 139.7 time_steps 16140 learning_steps 807\n",
      "episode 142 score 200.0 avg score 140.9 time_steps 16340 learning_steps 817\n",
      "episode 143 score 200.0 avg score 142.2 time_steps 16540 learning_steps 827\n",
      "episode 144 score 200.0 avg score 143.6 time_steps 16740 learning_steps 837\n",
      "episode 145 score 200.0 avg score 144.6 time_steps 16940 learning_steps 847\n",
      "episode 146 score 200.0 avg score 146.2 time_steps 17140 learning_steps 857\n",
      "episode 147 score 200.0 avg score 147.9 time_steps 17340 learning_steps 867\n",
      "episode 148 score 200.0 avg score 149.7 time_steps 17540 learning_steps 877\n",
      "episode 149 score 200.0 avg score 150.6 time_steps 17740 learning_steps 887\n",
      "episode 150 score 200.0 avg score 151.1 time_steps 17940 learning_steps 897\n",
      "episode 151 score 200.0 avg score 151.1 time_steps 18140 learning_steps 907\n",
      "episode 152 score 200.0 avg score 151.1 time_steps 18340 learning_steps 917\n",
      "episode 153 score 200.0 avg score 151.6 time_steps 18540 learning_steps 927\n",
      "episode 154 score 200.0 avg score 152.7 time_steps 18740 learning_steps 937\n",
      "episode 155 score 200.0 avg score 152.7 time_steps 18940 learning_steps 947\n",
      "episode 156 score 200.0 avg score 153.0 time_steps 19140 learning_steps 957\n",
      "episode 157 score 200.0 avg score 153.4 time_steps 19340 learning_steps 967\n",
      "episode 158 score 200.0 avg score 153.4 time_steps 19540 learning_steps 977\n",
      "episode 159 score 200.0 avg score 153.6 time_steps 19740 learning_steps 987\n",
      "episode 160 score 200.0 avg score 153.6 time_steps 19940 learning_steps 997\n",
      "episode 161 score 200.0 avg score 153.6 time_steps 20140 learning_steps 1007\n",
      "episode 162 score 200.0 avg score 153.6 time_steps 20340 learning_steps 1017\n",
      "episode 163 score 140.0 avg score 153.7 time_steps 20480 learning_steps 1024\n",
      "episode 164 score 200.0 avg score 155.4 time_steps 20680 learning_steps 1034\n",
      "episode 165 score 200.0 avg score 157.0 time_steps 20880 learning_steps 1044\n",
      "episode 166 score 200.0 avg score 157.7 time_steps 21080 learning_steps 1054\n",
      "episode 167 score 200.0 avg score 158.0 time_steps 21280 learning_steps 1064\n",
      "episode 168 score 200.0 avg score 158.7 time_steps 21480 learning_steps 1074\n",
      "episode 169 score 200.0 avg score 159.3 time_steps 21680 learning_steps 1084\n",
      "episode 170 score 200.0 avg score 160.0 time_steps 21880 learning_steps 1094\n",
      "episode 171 score 200.0 avg score 160.5 time_steps 22080 learning_steps 1104\n",
      "episode 172 score 200.0 avg score 162.0 time_steps 22280 learning_steps 1114\n",
      "episode 173 score 200.0 avg score 163.2 time_steps 22480 learning_steps 1124\n",
      "episode 174 score 200.0 avg score 164.2 time_steps 22680 learning_steps 1134\n",
      "episode 175 score 200.0 avg score 165.6 time_steps 22880 learning_steps 1144\n",
      "episode 176 score 200.0 avg score 167.2 time_steps 23080 learning_steps 1154\n",
      "episode 177 score 200.0 avg score 168.8 time_steps 23280 learning_steps 1164\n",
      "episode 178 score 200.0 avg score 168.8 time_steps 23480 learning_steps 1174\n",
      "episode 179 score 200.0 avg score 168.9 time_steps 23680 learning_steps 1184\n",
      "episode 180 score 167.0 avg score 169.2 time_steps 23847 learning_steps 1192\n",
      "episode 181 score 64.0 avg score 167.8 time_steps 23911 learning_steps 1195\n",
      "episode 182 score 84.0 avg score 167.6 time_steps 23995 learning_steps 1199\n",
      "episode 183 score 18.0 avg score 166.9 time_steps 24013 learning_steps 1200\n",
      "episode 184 score 53.0 avg score 167.0 time_steps 24066 learning_steps 1203\n",
      "episode 185 score 17.0 avg score 165.7 time_steps 24083 learning_steps 1204\n",
      "episode 186 score 47.0 avg score 165.6 time_steps 24130 learning_steps 1206\n",
      "episode 187 score 62.0 avg score 165.0 time_steps 24192 learning_steps 1209\n",
      "episode 188 score 59.0 avg score 164.0 time_steps 24251 learning_steps 1212\n",
      "episode 189 score 146.0 avg score 164.7 time_steps 24397 learning_steps 1219\n",
      "episode 190 score 184.0 avg score 165.2 time_steps 24581 learning_steps 1229\n",
      "episode 191 score 156.0 avg score 166.0 time_steps 24737 learning_steps 1236\n",
      "episode 192 score 147.0 avg score 166.9 time_steps 24884 learning_steps 1244\n",
      "episode 193 score 138.0 avg score 167.3 time_steps 25022 learning_steps 1251\n",
      "episode 194 score 136.0 avg score 166.8 time_steps 25158 learning_steps 1257\n",
      "episode 195 score 180.0 avg score 167.7 time_steps 25338 learning_steps 1266\n",
      "episode 196 score 198.0 avg score 169.1 time_steps 25536 learning_steps 1276\n",
      "episode 197 score 200.0 avg score 169.2 time_steps 25736 learning_steps 1286\n",
      "episode 198 score 200.0 avg score 170.7 time_steps 25936 learning_steps 1296\n",
      "episode 199 score 200.0 avg score 170.7 time_steps 26136 learning_steps 1306\n",
      "episode 200 score 200.0 avg score 170.7 time_steps 26336 learning_steps 1316\n",
      "episode 201 score 200.0 avg score 170.7 time_steps 26536 learning_steps 1326\n",
      "episode 202 score 200.0 avg score 170.7 time_steps 26736 learning_steps 1336\n",
      "episode 203 score 200.0 avg score 170.7 time_steps 26936 learning_steps 1346\n",
      "episode 204 score 200.0 avg score 170.7 time_steps 27136 learning_steps 1356\n",
      "episode 205 score 200.0 avg score 170.7 time_steps 27336 learning_steps 1366\n",
      "episode 206 score 200.0 avg score 170.7 time_steps 27536 learning_steps 1376\n",
      "episode 207 score 163.0 avg score 170.3 time_steps 27699 learning_steps 1384\n",
      "episode 208 score 128.0 avg score 169.6 time_steps 27827 learning_steps 1391\n",
      "episode 209 score 151.0 avg score 169.9 time_steps 27978 learning_steps 1398\n",
      "episode 210 score 97.0 avg score 168.9 time_steps 28075 learning_steps 1403\n",
      "episode 211 score 155.0 avg score 168.4 time_steps 28230 learning_steps 1411\n",
      "episode 212 score 145.0 avg score 168.6 time_steps 28375 learning_steps 1418\n",
      "episode 213 score 172.0 avg score 169.1 time_steps 28547 learning_steps 1427\n",
      "episode 214 score 200.0 avg score 169.1 time_steps 28747 learning_steps 1437\n",
      "episode 215 score 195.0 avg score 169.1 time_steps 28942 learning_steps 1447\n",
      "episode 216 score 200.0 avg score 169.1 time_steps 29142 learning_steps 1457\n",
      "episode 217 score 200.0 avg score 169.2 time_steps 29342 learning_steps 1467\n",
      "episode 218 score 191.0 avg score 170.9 time_steps 29533 learning_steps 1476\n",
      "episode 219 score 177.0 avg score 171.5 time_steps 29710 learning_steps 1485\n",
      "episode 220 score 200.0 avg score 171.5 time_steps 29910 learning_steps 1495\n",
      "episode 221 score 200.0 avg score 173.1 time_steps 30110 learning_steps 1505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 222 score 200.0 avg score 173.1 time_steps 30310 learning_steps 1515\n",
      "episode 223 score 200.0 avg score 173.1 time_steps 30510 learning_steps 1525\n",
      "episode 224 score 200.0 avg score 173.1 time_steps 30710 learning_steps 1535\n",
      "episode 225 score 200.0 avg score 173.1 time_steps 30910 learning_steps 1545\n",
      "episode 226 score 200.0 avg score 173.5 time_steps 31110 learning_steps 1555\n",
      "episode 227 score 200.0 avg score 174.2 time_steps 31310 learning_steps 1565\n",
      "episode 228 score 200.0 avg score 175.1 time_steps 31510 learning_steps 1575\n",
      "episode 229 score 200.0 avg score 175.1 time_steps 31710 learning_steps 1585\n",
      "episode 230 score 200.0 avg score 175.1 time_steps 31910 learning_steps 1595\n",
      "episode 231 score 200.0 avg score 175.1 time_steps 32110 learning_steps 1605\n",
      "episode 232 score 200.0 avg score 175.1 time_steps 32310 learning_steps 1615\n",
      "episode 233 score 54.0 avg score 173.7 time_steps 32364 learning_steps 1618\n",
      "episode 234 score 200.0 avg score 173.7 time_steps 32564 learning_steps 1628\n",
      "episode 235 score 200.0 avg score 175.0 time_steps 32764 learning_steps 1638\n",
      "episode 236 score 200.0 avg score 175.0 time_steps 32964 learning_steps 1648\n",
      "episode 237 score 200.0 avg score 175.9 time_steps 33164 learning_steps 1658\n",
      "episode 238 score 200.0 avg score 177.3 time_steps 33364 learning_steps 1668\n",
      "episode 239 score 200.0 avg score 178.2 time_steps 33564 learning_steps 1678\n",
      "episode 240 score 200.0 avg score 178.2 time_steps 33764 learning_steps 1688\n",
      "episode 241 score 200.0 avg score 178.2 time_steps 33964 learning_steps 1698\n",
      "episode 242 score 200.0 avg score 178.2 time_steps 34164 learning_steps 1708\n",
      "episode 243 score 200.0 avg score 178.2 time_steps 34364 learning_steps 1718\n",
      "episode 244 score 200.0 avg score 178.2 time_steps 34564 learning_steps 1728\n",
      "episode 245 score 200.0 avg score 178.2 time_steps 34764 learning_steps 1738\n",
      "episode 246 score 200.0 avg score 178.2 time_steps 34964 learning_steps 1748\n",
      "episode 247 score 200.0 avg score 178.2 time_steps 35164 learning_steps 1758\n",
      "episode 248 score 200.0 avg score 178.2 time_steps 35364 learning_steps 1768\n",
      "episode 249 score 200.0 avg score 178.2 time_steps 35564 learning_steps 1778\n",
      "episode 250 score 200.0 avg score 178.2 time_steps 35764 learning_steps 1788\n",
      "episode 251 score 43.0 avg score 176.7 time_steps 35807 learning_steps 1790\n",
      "episode 252 score 200.0 avg score 176.7 time_steps 36007 learning_steps 1800\n",
      "episode 253 score 200.0 avg score 176.7 time_steps 36207 learning_steps 1810\n",
      "episode 254 score 200.0 avg score 176.7 time_steps 36407 learning_steps 1820\n",
      "episode 255 score 200.0 avg score 176.7 time_steps 36607 learning_steps 1830\n",
      "episode 256 score 200.0 avg score 176.7 time_steps 36807 learning_steps 1840\n",
      "episode 257 score 200.0 avg score 176.7 time_steps 37007 learning_steps 1850\n",
      "episode 258 score 200.0 avg score 176.7 time_steps 37207 learning_steps 1860\n",
      "episode 259 score 200.0 avg score 176.7 time_steps 37407 learning_steps 1870\n",
      "episode 260 score 200.0 avg score 176.7 time_steps 37607 learning_steps 1880\n",
      "episode 261 score 200.0 avg score 176.7 time_steps 37807 learning_steps 1890\n",
      "episode 262 score 200.0 avg score 176.7 time_steps 38007 learning_steps 1900\n",
      "episode 263 score 200.0 avg score 177.3 time_steps 38207 learning_steps 1910\n",
      "episode 264 score 200.0 avg score 177.3 time_steps 38407 learning_steps 1920\n",
      "episode 265 score 200.0 avg score 177.3 time_steps 38607 learning_steps 1930\n",
      "episode 266 score 200.0 avg score 177.3 time_steps 38807 learning_steps 1940\n",
      "episode 267 score 200.0 avg score 177.3 time_steps 39007 learning_steps 1950\n",
      "episode 268 score 200.0 avg score 177.3 time_steps 39207 learning_steps 1960\n",
      "episode 269 score 200.0 avg score 177.3 time_steps 39407 learning_steps 1970\n",
      "episode 270 score 200.0 avg score 177.3 time_steps 39607 learning_steps 1980\n",
      "episode 271 score 200.0 avg score 177.3 time_steps 39807 learning_steps 1990\n",
      "episode 272 score 200.0 avg score 177.3 time_steps 40007 learning_steps 2000\n",
      "episode 273 score 200.0 avg score 177.3 time_steps 40207 learning_steps 2010\n",
      "episode 274 score 40.0 avg score 175.7 time_steps 40247 learning_steps 2012\n",
      "episode 275 score 200.0 avg score 175.7 time_steps 40447 learning_steps 2022\n",
      "episode 276 score 200.0 avg score 175.7 time_steps 40647 learning_steps 2032\n",
      "episode 277 score 200.0 avg score 175.7 time_steps 40847 learning_steps 2042\n",
      "episode 278 score 65.0 avg score 174.3 time_steps 40912 learning_steps 2045\n",
      "episode 279 score 39.0 avg score 172.7 time_steps 40951 learning_steps 2047\n",
      "episode 280 score 63.0 avg score 171.7 time_steps 41014 learning_steps 2050\n",
      "episode 281 score 30.0 avg score 171.3 time_steps 41044 learning_steps 2052\n",
      "episode 282 score 56.0 avg score 171.1 time_steps 41100 learning_steps 2055\n",
      "episode 283 score 73.0 avg score 171.6 time_steps 41173 learning_steps 2058\n",
      "episode 284 score 200.0 avg score 173.1 time_steps 41373 learning_steps 2068\n",
      "episode 285 score 200.0 avg score 174.9 time_steps 41573 learning_steps 2078\n",
      "episode 286 score 45.0 avg score 174.9 time_steps 41618 learning_steps 2080\n",
      "episode 287 score 200.0 avg score 176.3 time_steps 41818 learning_steps 2090\n",
      "episode 288 score 200.0 avg score 177.7 time_steps 42018 learning_steps 2100\n",
      "episode 289 score 200.0 avg score 178.2 time_steps 42218 learning_steps 2110\n",
      "episode 290 score 200.0 avg score 178.4 time_steps 42418 learning_steps 2120\n",
      "episode 291 score 200.0 avg score 178.8 time_steps 42618 learning_steps 2130\n",
      "episode 292 score 200.0 avg score 179.3 time_steps 42818 learning_steps 2140\n",
      "episode 293 score 200.0 avg score 180.0 time_steps 43018 learning_steps 2150\n",
      "episode 294 score 200.0 avg score 180.6 time_steps 43218 learning_steps 2160\n",
      "episode 295 score 200.0 avg score 180.8 time_steps 43418 learning_steps 2170\n",
      "episode 296 score 200.0 avg score 180.8 time_steps 43618 learning_steps 2180\n",
      "episode 297 score 200.0 avg score 180.8 time_steps 43818 learning_steps 2190\n",
      "episode 298 score 200.0 avg score 180.8 time_steps 44018 learning_steps 2200\n",
      "episode 299 score 200.0 avg score 180.8 time_steps 44218 learning_steps 2210\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = gym.make('CartPole-v0')\n",
    "    N = 20\n",
    "    batch_size = 5\n",
    "    n_epochs = 4\n",
    "    alpha = 0.0003\n",
    "    agent = Agent(n_actions=env.action_space.n, batch_size=batch_size, \n",
    "                    alpha=alpha, n_epochs=n_epochs, \n",
    "                    input_dims=env.observation_space.shape)\n",
    "    n_games = 300\n",
    "\n",
    "    figure_file = 'plots/cartpole.png'\n",
    "\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "\n",
    "    learn_iters = 0\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "\n",
    "    for i in range(n_games):\n",
    "        observation = env.reset()\n",
    "        env.render()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action, prob, val = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            n_steps += 1\n",
    "            score += reward\n",
    "            agent.remember(observation, action, prob, val, reward, done)\n",
    "            if n_steps % N == 0:\n",
    "                agent.learn()\n",
    "                learn_iters += 1\n",
    "            observation = observation_\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            #agent.save_models()\n",
    "\n",
    "        print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score,\n",
    "                'time_steps', n_steps, 'learning_steps', learn_iters)\n",
    "    x = [i+1 for i in range(len(score_history))]\n",
    "    plot_learning_curve(x, score_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6316644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
